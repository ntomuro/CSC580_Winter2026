{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMgI6h5OF8tNHh76DSmxEos"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## CSC 580 AI II (Winter 2026) **HW\\#4 Atari Pong** -- Start-up code\n","### **Pong_train.ipynb** -- Perform further training"],"metadata":{"id":"6puMM88Cbxb6"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29bJ5Pk2hzgL","executionInfo":{"status":"ok","timestamp":1768759930683,"user_tz":360,"elapsed":20080,"user":{"displayName":"Noriko T","userId":"06082512421306527471"}},"outputId":"55be29b6-1665-494d-ad14-795d9abced84"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["## Code piece to mount my Google Drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\") # my Google Drive root directory will be mapped here"]},{"cell_type":"code","source":["# Change the working directory to your own work directory (where the code file is).\n","import os\n","thisdir = '/content/drive/My Drive/CSC580_Winter2026/Atari_Pong'\n","os.chdir(thisdir)\n","\n","# Ensure the files are there (in the folder)\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qV2p-JvSh0WV","executionInfo":{"status":"ok","timestamp":1768759939504,"user_tz":360,"elapsed":1419,"user":{"displayName":"Noriko T","userId":"06082512421306527471"}},"outputId":"5f5c02ba-0234-4146-c89e-d879af7a70ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/CSC580_Winter2026/Atari_Pong\n"]}]},{"cell_type":"markdown","source":["## Install relevant libraries"],"metadata":{"id":"spxoFbjeqz1u"}},{"cell_type":"code","source":["%pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAVJfLxJqkX0","executionInfo":{"status":"ok","timestamp":1768759949035,"user_tz":360,"elapsed":5572,"user":{"displayName":"Noriko T","userId":"06082512421306527471"}},"outputId":"a9e591ae-942b-4c47-95c0-fdacde222fe8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: ale_py in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (0.11.2)\n","Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (1.2.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (3.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.12.0.88)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (2.9.0+cu126)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->-r requirements.txt (line 2)) (3.1.2)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium->-r requirements.txt (line 2)) (4.15.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium->-r requirements.txt (line 2)) (0.0.4)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (4.61.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (3.3.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 3)) (2.9.0.post0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.20.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.6.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 6)) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3)) (1.17.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 6)) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->-r requirements.txt (line 6)) (3.0.3)\n"]}]},{"cell_type":"code","source":["import dqn_core # dqn_core.py\n","from dqn_core import AtariPreprocess, FrameStack, PongActionReducer, ReplayBuffer, DQN, make_env\n","import gymnasium as gym\n","import numpy as np\n","import random\n","import pickle"],"metadata":{"id":"8jk90gLYyawq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create an ale  (for display)"],"metadata":{"id":"X0Rgcpboq6LT"}},{"cell_type":"code","source":["from ale_py import ALEInterface\n","ale = ALEInterface()"],"metadata":{"id":"v3JBxeY3nqVp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. Create a DQN model (prediction/q_network and target_network) and load the pre-trained weights.  Also create the replay buffer and load the saved transitions.\n","\n","Assume the q_net weight file is called **\"q_net_XXXX.pt\"** (where XXXX is the number of training steps, e.g. '700k' and '1M'), and it is found under the folder \"checkpoints\".  Note that this code is the same as the one in the evaluation code."],"metadata":{"id":"Yo_fSrlw2tjk"}},{"cell_type":"markdown","source":["### Functions to load pretrained weights and replay buffer"],"metadata":{"id":"aXFp9t0Ni2Wn"}},{"cell_type":"code","source":["def build_networks(n_actions, device):\n","    \"\"\"Build the prediction and target networks.\"\"\"\n","    q_net = DQN(n_actions).to(device)      # prediction network\n","    target_net = DQN(n_actions).to(device) # (frozen) target network\n","    return q_net, target_net\n","\n","def load_pretrained(q_net, target_net, q_file, t_file, device):\n","    \"\"\"Load pretrained weights from files for q_net and target_net.\n","       Move the tensors to device when loading.\"\"\"\n","    q_net.load_state_dict(torch.load(q_file, map_location=device))\n","    target_net.load_state_dict(torch.load(t_file, map_location=device))\n","    target_net.eval() # target network does not 'learn'\n","    print(f\"✅ Loaded pretrained networks from checkpoints\")\n","\n","def load_replay_buffer(pklfilepath):\n","    \"\"\"Load replay buffer.  Assuming a .pkl file.\"\"\"\n","    replay = ReplayBuffer.load(pklfilepath)\n","    #with open(pklfilepath, \"rb\") as f:\n","    #    replay = pickle.load(f)\n","    print(f\"✅ Loaded replay buffer with {len(replay)} transitions\")\n","    return replay\n"],"metadata":{"id":"6JRiwhErB0fU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Initial** calls to those functions to load models and reply buffer.  Also create an environment."],"metadata":{"id":"WuKsc4qjE397"}},{"cell_type":"code","source":["import torch # define GPU access\n","DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# folder and filenames for pretrained weights and reply buffer\n","checkpoint_dir = \"checkpoints\"\n","\n","# (*) Initial checkpoint info\n","START_STEP = 700_000\n","str_steps = \"_700k\"\n","\n","qnet_file = f\"{checkpoint_dir}/q_net{str_steps}.pt\"\n","target_file = f\"{checkpoint_dir}/target_net{str_steps}.pt\"\n","replay_file = f\"{checkpoint_dir}/replay{str_steps}.pkl\"\n","\n","#------------------------------\n","# create an environment\n","env = make_env() # default env; function in dqn_core.py\n","n_actions = env.action_space.n\n","print(f\"✅ Number of actions: {n_actions}\")\n","\n","# load pretrained model weights\n","q_net, target_net = build_networks(n_actions, DEVICE)\n","load_pretrained(q_net, target_net, qnet_file, target_file, DEVICE)\n","\n","# Define optimizer here, after q_net is instantiated\n","import torch # Ensure torch is imported if not already\n","optimizer = torch.optim.Adam(q_net.parameters(), lr=1e-4)\n","\n","# Load the pre-loaded replay buffer\n","buffer = load_replay_buffer(replay_file)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FA_r32_s_z9k","executionInfo":{"status":"ok","timestamp":1768760092533,"user_tz":360,"elapsed":34168,"user":{"displayName":"Noriko T","userId":"06082512421306527471"}},"outputId":"4a26c9df-74c4-4a3b-ea24-25f55c4789bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Number of actions: 4\n","✅ Loaded pretrained networks from checkpoints\n","✅ Loaded replay buffer with 100000 transitions\n"]}]},{"cell_type":"markdown","source":["## 2. Further training\n","### 2.0 Declare global constants"],"metadata":{"id":"plOfxk0utEKg"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Hyperparameters (students may tune)\n","GAMMA = 0.99\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 100_000\n","LEARNING_RATE = 1e-4\n","TARGET_UPDATE_FREQ = 10_000\n","LEARNING_STARTS = 50_000\n","TOTAL_STEPS = 300_000\n","\n","EPS_START = 1.0\n","EPS_END = 0.1\n","EPS_DECAY = 1_500_000  # divisor to the current steps (e.g. 700,000 / 1,500,000 = 0.467)"],"metadata":{"id":"PrMNdocntId0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.1 select_action -- Epsilon-greedy strategy"],"metadata":{"id":"GBk0n1TttPwD"}},{"cell_type":"code","source":["def select_action(state, current_steps):\n","    ## (*) TODO:\n","    ## Implement epsilon-greedy exploration.  It computes the epsilon as\n","    ## current steps divided by EPS_DECAY subtracted from EPS_START, or\n","    ## EPS_END, whichever the largest.\n","    ## Hint: Look at the evaluation section for the code for greedy choice.\n","    ##\n","\n"],"metadata":{"id":"alzpOK-FtTHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.2 Optimization (and backprop) step"],"metadata":{"id":"2tDIcDdxuA0M"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","def optimize():\n","    if len(buffer) < BATCH_SIZE:\n","        return\n","\n","    ## (*) TODO\n","    ## Obtain BATCH_SIZE number of transitions from the reply buffer.\n","    ## states, actions, rewards, next_states, dones = ...\n","    ##\n","\n","    states = torch.tensor(states, device=DEVICE)\n","    actions = torch.tensor(actions, device=DEVICE).unsqueeze(1)\n","    rewards = torch.tensor(rewards, device=DEVICE)\n","    next_states = torch.tensor(next_states, device=DEVICE)\n","    # Convert dones to float32\n","    dones = torch.tensor(dones, dtype=torch.float32, device=DEVICE)\n","\n","    # Q(s, a): From the Q-network’s output, pick the Q-value corresponding\n","    # to the action that was actually taken, for each state in the batch.\n","    # 'q_net(states)' where states is of shape (B, 4, 84, 84), returns\n","    # q-values of the form '[[ 1.2, -0.4,  0.7,  3.5], [ 0.1,  0.9, -0.2,  0.4],..'\n","    # 'gather(1,actions)' returns [[3.5], [0.9],..], and squeeze(1) makes\n","    # into a 1-D array [3.5, 0.9,..]\n","    q_values = q_net(states).gather(1, actions).squeeze(1)\n","\n","    # max_a' Q_target(s', a')\n","    with torch.no_grad():\n","        next_q_values = target_net(next_states).max(1)[0]\n","        ## (*) TODO\n","        ## Compute the target values (based on the algorithm).\n","        ## target = ...\n","        ##\n","\n","    # compute the loss\n","    loss = F.smooth_l1_loss(q_values, target)\n","\n","    # backprop\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n"],"metadata":{"id":"aR__pqetuG9x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Main Training Loop"],"metadata":{"id":"ZXQ8_s5CvbJr"}},{"cell_type":"code","source":["def do_train(start_step):\n","    episode_reward = 0\n","\n","    obs, info = env.reset()\n","    # FIRE to start Pong\n","    obs, _, _, _, _ = env.step(1)\n","\n","    for step in range(start_step + 1, start_step + TOTAL_STEPS):\n","        if step%1000 == 0:\n","          print (f\"Step {step}\")\n","\n","        ## (*) TODO\n","        ## Select action based on the epsilon-greedy policy.\n","        ## action = ...\n","\n","        # take the action\n","        next_obs, reward, terminated, truncated, info = env.step(action)\n","        done = terminated or truncated\n","\n","        ## (*) TODO:\n","        ## Store transition in replay buffer\n","        ##\n","\n","        obs = next_obs\n","        episode_reward += reward\n","\n","        #if step > LEARNING_STARTS:\n","        optimize()\n","\n","        ## (*) TODO\n","        ## Periodically update target network (TARGET_UPDATE_FREQ).\n","        ##\n","        ##\n","        ##\n","\n","        if done:\n","            print(f\"Step {step}: episode reward = {episode_reward}\")\n","            obs, info = env.reset()\n","            # FIRE to start Pong\n","            obs, _, _, _, _ = env.step(1)\n","            episode_reward = 0\n","\n","    return"],"metadata":{"id":"GAqx-8I_vkg8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3.1 **Run do_train()** by executing the cell below."],"metadata":{"id":"gOjf3ZpLdddL"}},{"cell_type":"code","source":["# Train further TOTAL_STEPS number of steps\n","do_train(START_STEP)"],"metadata":{"id":"MG_COIsBX0RF","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Visualize the further-trained model (one episode)"],"metadata":{"id":"XWG30eCddfAF"}},{"cell_type":"code","source":["from IPython.display import display\n","import matplotlib.pyplot as plt\n","import torch\n","import time\n","\n","def visualize_one_episode(model, env):\n","    obs, info = env.reset()\n","    obs, _, _, _, _ = env.step(1)  # FIRE to start the game\n","\n","    # set up the visualization variables (fig, img, ax)\n","    fig, ax = plt.subplots(figsize=(4, 4))\n","    img = ax.imshow(env.render())\n","    ax.axis(\"off\")\n","\n","    display_handle = display(fig, display_id=True) # set up display_handle\n","\n","    frames = [] # collect frames for a video\n","\n","    # (*) start an episode (until either player reaches 21 points)\n","    # actions were chosen by the greedy strategy, selecting the action with the\n","    # largest value produced on the output layer of the model (q_net).\n","    done = False\n","    total_reward = 0\n","\n","    while not done:\n","        with torch.no_grad():\n","            # tranform obs (4, 84, 84) to a batch (of one instance)\n","            state = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n","\n","            # (*) select action with the highest Q-value for the current state.\n","            action = model(state).argmax(1).item()\n","            # q_net(state) returns model output, which is a tensor containing\n","            # values of the actions, e.g. 'tensor([[ 1.2, -0.4,  0.7,  3.5]])',\n","            # where the inner list is the average rewards of the actions.\n","            # Then .argmax(1) returns 'tensor([3])', then by .item(), we get 3,\n","            # the action number of the 'best' action.\n","\n","        # take the action and receive info from the environment\n","        obs, reward, terminated, truncated, _ = env.step(action)\n","        done = terminated or truncated\n","        total_reward += reward\n","\n","        # visualization\n","        frame = env.render()\n","        frames.append\n","        img.set_data(frame)\n","        display_handle.update(fig)\n","\n","        time.sleep(0.03)\n","\n","    plt.close(fig)\n","    env.close()\n","\n","    print(\"Total reward:\", total_reward)\n","    return frames, total_reward\n"],"metadata":{"id":"SPbErATuX0SN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Call the visualization function (with the last model/q_net)\n","env = make_env(render_mode=\"rgb_array\") # for visual rendering\n","\n","model = DQN(n_actions)\n","model.load_state_dict(q_net.state_dict())\n","model.eval()\n","\n","frames, total_reward = visualize_one_episode(q_net, env)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"wibTXsdLeOmh","executionInfo":{"status":"ok","timestamp":1768765713420,"user_tz":360,"elapsed":280238,"user":{"displayName":"Noriko T","userId":"06082512421306527471"}},"outputId":"9925ef0b-e5a1-41ea-b8b2-a008ddaf5e28"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 400x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Total reward: -20.0\n"]}]},{"cell_type":"markdown","source":["## 5. Save the model (q_net and target_net) and the replay buffer"],"metadata":{"id":"O0Tp_JfywC9o"}},{"cell_type":"code","source":["steps = \"_1M\"\n","qnet_file = f\"{checkpoint_dir}/q_net{steps}.pt\"\n","target_file = f\"{checkpoint_dir}/target_net{steps}.pt\"\n","replay_file = f\"{checkpoint_dir}/replay{steps}.pkl\"\n","\n","torch.save(q_net.state_dict(), qnet_file)\n","torch.save(target_net.state_dict(), target_file)\n","buffer.save(replay_file)\n","\n","env.close()"],"metadata":{"id":"xIR6JzjBv57c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Create a video and save it"],"metadata":{"id":"h7RgfwF0k8sC"}},{"cell_type":"code","source":["import imageio\n","\n","video_dir = \"videos\"\n","video_path = f\"{video_dir}/pong{steps}.mp4\"\n","imageio.mimsave(video_path, frames, fps=30)\n","print(f\"Saved video to {video_path}\")\n"],"metadata":{"id":"I8hefIQDv6N7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1768765802303,"user_tz":360,"elapsed":70,"user":{"displayName":"Noriko T","userId":"06082512421306527471"}},"outputId":"3bd91198-d68c-42af-a9d3-3815530f3bba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved video to videos/pong_1M.mp4\n"]}]}]}